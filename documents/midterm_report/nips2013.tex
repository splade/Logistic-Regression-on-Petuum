\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amsmath}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09


\title{Large-Scale L1-Regularized and L2-Regularized Logistic Regression via a Stale Synchronous Parallel Parameter Server}


\author{
David S.~Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

L1-regularized and L2-regularized logistic regression (LR) are widely used machine learning models that underlie many important applications in industry. A particular one that successfully applies these models is online advertising, including search advertising, contextual advertising, display advertising, and real-time bidding auctions [4]. According to eMarketer [10], the worldwide internet ad spending breaks the 100-billion mark in 2012 and is predicted to continue increasing by more than 20\% per year. Moreover, internet advertising is still the dominating revenue source for most search engines [7]. Most notably, Google earned \$36.4 billion from advertising alone, accounting for 96\% of its total revenue in 2011 [11].

The most common financial model, adopted by Google, Yahoo, and Microsoft [7], for online advertising is called “cost-per-click” billing. Essentially, the model states that advertisers (sponsors) pay the search engine company whenever a user clicks their ads [7]. As a result, the ability to efficiently and adaptively learn models that can quickly and accurately predict ad click-through rates has been playing a key role of revenue boost for those companies [4].

Three practical issues need to be resolved for ad click prediction systems to achieve good performance: sparsity, scalability and adptiveness. First, in such applications, the number of features is usually very large, resulting in high-dimensional data (examples). However, since the number of training examples is limited, logistic regression tends to overfit. A common way to reduce overfitting is through regularization, namely L1-norm and L2-norm regularization. The L2-norm regularization has the advantage of being able to reduce to a convex optimization problem that is always differentiable, while the L1-norm regularization is more attractive for dealing with high-dimensional data because it implicitly embeds feature selection in the classification process [3].

The second practical issue is scalability. Scalability is important to industrial applications because of both massive data volume and massive model size [1]. A typical industrial application nowadays is required to learn and predict billions of instances with high-dimensional feature space per day [4]. Meanwhile, observations show that increasing the scale of machine learning models can significantly improve classification accuracy [5]. The most natural way to address scalability is through parallelism. A number of distributed systems specifically optimized for machine learning have emerged recently [1] [2] [5], along with some theoretical work on consistency models for distributed ML [1] [9]. We will discuss some of these systems in sections 2. For this project, we use Petuum [2], an open source large-scale ML framework currently being developed at CMU, as the centralized shared parameter server to apply iterative-convergent methods to realize regularized LR models in large scale.

The final practical issue to be considered is adaptiveness, which means that the existing model is periodically refined through learning of the data obtained from the recent prediction results. Adaptiveness typically requires online algorithms where data is provided by a streaming service. In other words, each training example only needs to be considered once.

Despite that large-scale regularized logistic regression is widely adopted in proprietary systems, there is a lack of open source implementation. This project aims to fill in this need by implementing large-scale L2-regularized LR using stochastic gradient ascent (descent) and L1-regularized LR using subgradient method on a distributed ML framework, namely Petuum [2]. Our contribution also includes providing first-hand experience with Petuum, which might be valuable for Petuum to evolve to a mature ML-specific distributed system. Section 2 provides background and related work of LR algorithms and parallel (distributed) ML frameworks; Section 3 describes our algorithms and implementation details; Section 4 evaluates the performance of our system; Section 5 concludes. 

\section{Related Work}
\label{gen_inst}



\section{Methods}
\label{headings}

In this section, we first present a derivation of the algorithms chosen to implement large-scale L2-regularized and L1-regularized LR respectively. We then describes our system design and implementation based on Petuum [2] in detail.

\subsection{Stochastic Gradient Ascent (Descent) for Logistic Regression}

Suppose that the training set has n examples $\vec{x_1}$ to $\vec{x_n}$, each having a dimension of d. Let $y_1$ to $y_n$ denote the corresponding binary labels, i.e. $y_i \in \{0, 1\}$ for $\forall i = 1, ..., n$. The logistic function is defined as $\sigma(z) = \frac{1}{1+e^{-z}}$. The parameters associated with the LR model are $w_0$ and $\vec{w} = (w_1, ..., w_d)$. Let ($\vec{x}$, $y$) be a test sample, then we have the following probability model for logistic regression:

\begin{align*}
    p = P(y=1 | \vec{x}; w_0, \vec{w}) &= \sigma(w_0 + \sum\limits_{j=1}^d w_jx_j) = \frac{1}{1 + e^{-(w_0 + \vec{w}^\top\vec{x})}}\\
    P(y=0 | \vec{x}; w_0, \vec{w}) &= 1 - p = \frac{e^{-(w_0 + \vec{w}^\top\vec{x})}}{1 + e^{-(w_0 + \vec{w}^\top\vec{x})}}
\end{align*}


Using the above probablity model, we can compute the log odds as the following:

\begin{equation}
  ln\frac{p}{1-p} = w_0 + \vec{w}^\top\vec{x}
\end{equation}

and thus we have the classification rule for LR:
\begin{align*}
  y &= 1, \ \ \ \ \ if \ w_0 + \vec{w}^\top\vec{x} \geq 1\\
  y &= 0, \ \ \ \ \ otherwise.
\end{align*}

Let $\vec{w^*} = (w_0, \vec{w})$ and $\vec{x^*} = (1, \vec{x})$. Now the classification criteria becomes whether $\vec{w^*}^\top\vec{x^*} \geq 1$ or not. For notation simplicity, we denete $\vec{w^*}$ and $\vec{x^*}$ by $\vec{w}$ and $\vec{x}$ respectively from now on.

Given the training set, we learn the LR parameters using MLE (Maximum Likelyhood Estimator) method. Specifically, we want to maximize the following log joint conditional likelihood:

\begin{equation}
  LJCL = ln\prod\limits_{i=1}^nP(y_i | \vec{x_i}; \vec{w}) = \sum\limits_{i=1}^nlnP(y_i | \vec{x_i}; \vec{w})
\end{equation}

For each parameter $w_j$, we set the partial derivative of LJCL with respect to $w_j$ to 0:

\begin{equation}
\begin{split}
  \frac{\partial}{\partial w_j} LJCL &= \sum\limits_{i=1}^n\frac{\partial}{\partial w_j}lnP(y_i | \vec{x_i}; \vec{w})\\
  &= \sum\limits_{i:y_i=1}\frac{\partial}{\partial w_j}lnp_i + \sum\limits_{i:y_i=0}\frac{\partial}{\partial w_j}ln(1-p_i)\\
  &= \sum\limits_{i:y_i=1}\frac{1}{p_i}\frac{\partial}{\partial w_j}p_i + \sum\limits_{i:y_i=0}\frac{1}{1-p_i}(-\frac{\partial}{\partial w_j}(1-p_i))\\
  &= \sum\limits_{i:y_i=1}(1+e^{-\vec{w}^\top\vec{x_i}})\frac{-1}{(1+e^{-\vec{w}^\top\vec{x_i}})^2}\frac{\partial}{\partial w_j}e^{-\vec{w}^\top\vec{x_i}} + \sum\limits_{i:y_i=0}\frac{(1+e^{-\vec{w}^\top\vec{x_i}})}{e^{-\vec{w}^\top\vec{x_i}}}\frac{1}{(1+e^{-\vec{w}^\top\vec{x_i}})^2}\frac{\partial}{\partial w_j}e^{-\vec{w}^\top\vec{x_i}}\\
  &= \sum\limits_{i:y_i=1}\frac{e^{-\vec{w}^\top\vec{x_i}}}{(1+e^{-\vec{w}^\top\vec{x_i}})^2}x_{ij} + \sum\limits_{i:y_i=0}\frac{-1}{(1+e^{-\vec{w}^\top\vec{x_i}})^2}x_{ij}\\
  &= \sum\limits_{i:y_i=1}(1-p_i)x_{ij} + \sum\limits_{i:y_i=0}(-p_i)x_{ij}\\
  &= \sum\limits_{i=1}^n(y_i-p_i)x_{ij} = 0
\end{split}
\end{equation}

\subsubsection{L2 Regularization}



\subsubsection{L1 Regularization}

\subsection{Petuum?}

\section{Experiments}
\label{others}

\section{Conclusion}


\subsubsection*{References}

\small{
[1] Alexander, J.A. \& Mozer, M.C. (1995) Template-based algorithms
for connectionist rule extraction. In G. Tesauro, D. S. Touretzky
and T.K. Leen (eds.), {\it Advances in Neural Information Processing
Systems 7}, pp. 609-616. Cambridge, MA: MIT Press.

[2] Bower, J.M. \& Beeman, D. (1995) {\it The Book of GENESIS: Exploring
Realistic Neural Models with the GEneral NEural SImulation System.}
New York: TELOS/Springer-Verlag.

[3] Hasselmo, M.E., Schnell, E. \& Barkai, E. (1995) Dynamics of learning
and recall at excitatory recurrent synapses and cholinergic modulation
in rat hippocampal region CA3. {\it Journal of Neuroscience}
{\bf 15}(7):5249-5262.

[10] http://www.emarketer.com/Article/Digital-Account-One-Five-Ad-Dollars/1009592

[11] http://searchenginewatch.com/article/2140712/How-Google-Made-37.9-Billion-in-2011
}

\end{document}
